{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from collections import namedtuple, Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up Twitter credentials using tweepy \n",
    "\n",
    "auth = tweepy.OAuthHandler('UEALJgD2o5lwpOeAeNhb9ceWX', 'JqB4i0TIqRizGtp97BovQN5iQDgA5BpF5tHHL0nZkUEANjxJuV')\n",
    "auth.set_access_token('732602583689367552-KeUxTRNYO2XwFux13Tp1yLoKyylMyO7', '0WWtEXmjOV1g732CBPs9Te8xQa7VKuSIBP9A0J4dxm1mL')\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use named tuples to keep info of interest for each tweet; keep urls to remove from text later\n",
    "\n",
    "Tweet = namedtuple('Tweet', ['text', 'tags', 'cited', 'urls', 'author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions to collect info of interest\n",
    "\n",
    "def get_hashtags(tweet):\n",
    "    tags = []\n",
    "    if len(tweet['entities']['hashtags']) > 0:\n",
    "        for tag in tweet['entities']['hashtags']:\n",
    "            tags.append(tag['text'])\n",
    "    return tags\n",
    "\n",
    "def get_citations(tweet):\n",
    "    citations = []\n",
    "    if len(tweet['entities']['user_mentions']) > 0:\n",
    "        for mention in tweet['entities']['user_mentions']:\n",
    "            citations.append(mention['screen_name'])\n",
    "    return citations\n",
    "\n",
    "def get_urls(tweet):\n",
    "    urls = []\n",
    "    if len(tweet['entities']['urls']) > 0:\n",
    "        for url in tweet['entities']['urls']:\n",
    "            urls.append(url['url'])\n",
    "    return urls\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b336ff3f7331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_timeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'extended'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mkeepers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'retweeted_status'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         all_tweets[name] = [Tweet(tweet['full_text'], get_hashtags(tweet), get_citations(tweet), get_urls(tweet),\n\u001b[1;32m     11\u001b[0m                                   name) for tweet in keepers[:500]]    \n",
      "\u001b[0;32m<ipython-input-7-b336ff3f7331>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_timeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'extended'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mkeepers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'retweeted_status'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         all_tweets[name] = [Tweet(tweet['full_text'], get_hashtags(tweet), get_citations(tweet), get_urls(tweet),\n\u001b[1;32m     11\u001b[0m                                   name) for tweet in keepers[:500]]    \n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m                                                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                                                 \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                                                 proxies=self.api.proxy)\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTweepError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to send request: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    516\u001b[0m         }\n\u001b[1;32m    517\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m                 )\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/thinkful/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pull in tweets for each influencer listed in influencers.txt and process most recent 500 that are not retweets\n",
    "# store as lists in a dict with the influencer handles as keys \n",
    "\n",
    "all_tweets = {}\n",
    "with open('influencers.txt', 'r') as names:\n",
    "    for raw_name in names:\n",
    "        name = raw_name.strip()\n",
    "        tweets = tweepy.Cursor(api.user_timeline, id=name, tweet_mode='extended').items(800)\n",
    "        keepers = [tweet._json for tweet in tweets if 'retweeted_status' not in tweet._json]\n",
    "        all_tweets[name] = [Tweet(tweet['full_text'], get_hashtags(tweet), get_citations(tweet), get_urls(tweet),\n",
    "                                  name) for tweet in keepers[:500]]    \n",
    "\n",
    "names = all_tweets.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# also get the description each person uses\n",
    "\n",
    "descriptions = {}\n",
    "for name in names:\n",
    "    descriptions[name] = api.get_user(name)._json['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save data in case kernel hangs or is stopped\n",
    "\n",
    "with open('twitter_data.pydat', 'wb') as datafile:\n",
    "    pickle.dump(all_tweets, datafile)\n",
    "\n",
    "with open('twitter_descriptions.pydat', 'wb') as datafile:\n",
    "    pickle.dump(all_tweets, datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dict of Counters for the hashtags used by each person\n",
    "\n",
    "all_tags = {}\n",
    "for name in names:\n",
    "    tags = [tweet.tags for tweet in all_tweets[name]]\n",
    "    all_tags[name] = Counter(list(itertools.chain(*tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dict of Counters for the mentions made by each person\n",
    "\n",
    "all_mentions = {}\n",
    "for name in names:\n",
    "    mentions = [tweet.cited for tweet in all_tweets[name]]\n",
    "    all_mentions[name] = Counter(list(itertools.chain(*mentions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Counter for all the hashtags used\n",
    "\n",
    "tags = Counter()\n",
    "for name in names:\n",
    "    tags += all_tags[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all hashtags used by more than one influencer for use as features\n",
    "# should have done this with the hashtags too\n",
    "\n",
    "tag_features = [tag for tag in list(tags.keys()) if sum([0 if all_tags[name][tag] == 0 else 1 for name in names]) > 1]\n",
    "len(tag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Counter for all the mentions made\n",
    "\n",
    "mentions = Counter()\n",
    "for name in names:\n",
    "    mentions += all_mentions[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all mentions made by more than one influencer for use as features\n",
    "\n",
    "mention_features = [mention for mention in list(mentions.keys()) if sum([0 if all_mentions[name][mention] == 0 else 1\n",
    "                                                                         for name in names]) > 1]\n",
    "len(mention_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates between tags and mentions\n",
    "\n",
    "duplicates = list(set(tag_features) & set(mention_features))\n",
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from mentions since there are more of them\n",
    "# Note: this turned out to be unnecssary because tokenization split off the '#' but not the '@'\n",
    "\n",
    "mention_features = [mention for mention in mention_features if mention not in duplicates]\n",
    "len(mention_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create feature vectors comprising the counts for each tag and mention feature\n",
    "\n",
    "entities = pd.DataFrame(index=names, columns=(tag_features + mention_features))\n",
    "for name in names:\n",
    "    for tag in tag_features:\n",
    "        entities.loc[name, tag] = all_tags[name][tag]\n",
    "    for mention in mention_features:\n",
    "        entities.loc[name, mention] = all_tags[name][mention]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth, SpectralClustering, DBSCAN\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the features and perform a 2-component PCA analysis to use for graphing\n",
    "\n",
    "normed_ents = normalize(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_pca = PCA(2).fit(normed_ents)\n",
    "ents_pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try basic mean shift clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MeanShift()\n",
    "ms.fit(normed_ents)\n",
    "ms.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean shift did not produce usable results, merely clustering all the authors together except for four clusters of one person each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using silhouette scores over 1,000 runs to find the optimal number of clusters, then the best clusters\n",
    "\n",
    "clusters = []\n",
    "for _ in range(1000):\n",
    "    best_score = 0\n",
    "    num_labels = 0\n",
    "    for num_clusters in range(2, 7):\n",
    "        km = KMeans(num_clusters)\n",
    "        km.fit(normed_ents)\n",
    "        sil_score = silhouette_score(normed_ents, km.labels_)\n",
    "        if sil_score > best_score:\n",
    "            best_score = sil_score\n",
    "            num_labels = len(np.unique(km.labels_))\n",
    "    clusters.append(num_labels)\n",
    "\n",
    "optimal_num_clusters = Counter(clusters).most_common(1)[0][0]\n",
    "print('The optimal number of clusters is {}'.format(optimal_num_clusters))\n",
    "      \n",
    "best_labels = []\n",
    "best_score = 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    km = KMeans(optimal_num_clusters)\n",
    "    km.fit(normed_ents)\n",
    "    sil_score = silhouette_score(normed_ents, km.labels_)\n",
    "    if sil_score > best_score:\n",
    "        best_score = sil_score\n",
    "        best_labels = km.labels_\n",
    "\n",
    "for i in range(optimal_num_clusters):\n",
    "    print('Cluster {} comprises {}'.format(i, [name for ix, name in enumerate(names) if best_labels[ix] == i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(clusters).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm generated one cluster of authors with very few hashtags and mentions, and another comprising everyone else. This is a valid but not very sophisticated breakdown. The only really interesting result is that one author with a relatively high number of tags and mentions (EvanSinar, with 665) was put in the group with low numbers. Apparently he had few of these entities in common with others, resulting in low similarity scores as did authors with few entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print('{:15}: {} entities'.format(name, entities.loc[name].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Twitter account descriptions are not very helpful for discovering similarities. Most are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = pd.DataFrame(index=names, columns=['description', 'label'])\n",
    "for name in names:\n",
    "    groups.loc[name, 'description'] = descriptions[name]\n",
    "groups.loc[:, 'label'] = best_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in np.unique(best_labels):\n",
    "    for name in names:\n",
    "        if groups.loc[name, 'label'] == label:\n",
    "            print(name, ':  ', groups.loc[name, 'description'], '\\n')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the clusters against the two principal components for a simple visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viz = ents_pca.transform(entities)\n",
    "viz = pd.DataFrame(viz, index=names, columns=['pca0', 'pca1'])\n",
    "viz['label'] = best_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.cubehelix_palette(n_colors=2, rot=-0.7)\n",
    "sns.relplot(x='pca0', y='pca1', hue='label', data=viz, palette=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the plot utiizes only the first two components, which combine to explain less than half of the variance, and a few points are not individually visible since they are too close together, even here the clusters can be seen to comprise a large group occupying most of the plot and a group of outliers. With the exception of EvanSinar, the outlying cluster groups together people who included zero or very few hashtags and mentions common to others in their tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz['num_entities'] = [sum(list(entities.iloc[ix])) for ix, name in enumerate(names)]\n",
    "\n",
    "viz.sort_values(by='num_entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal KMeans grouping scheme of 2 clusters divides the authors into a group with very few common hashtags and mentions, and another group comprising authors with many common hashtags and mentions. EvanSinar is the exception, having 665 common hashtags and mentions but appearing in the group with very few of these. This might represent one network comprising 9 members and 6 people whose networks do not intersect with each other or those in the 9-member group, but the few-entities group also might comprise mostly authors whose style is to not use many hashtags or mentions. It would be interesting to look at the 6-cluster groupings, but this question cannot be settled given the data. Without all of the authors using a good number of common hashtags and/or mentions, clustering on these features will not produce useful results. Clustering on only those authors who do use large numbers of these entities could produce insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SpectralClustering(n_clusters=4)\n",
    "sc.fit(entities)\n",
    "\n",
    "sc_labels = sc.labels_\n",
    "sc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning arises from the fact that two of the authors have entity vectors that are all zeroes. This results in the graph being not fully connected. Forcing the algorithm to create four clusters produces one large cluster, two 2-person clusters and a 1-person cluster. At this point, the reults taken together indicate that clustering on the hashtags and mentions is not very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try clustering using tf-idf vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather the tweet texts as elements of a list in preparation for tf-idf vectorization\n",
    "# create a dataframe with a list for each author, for clustering, and one with indidvidual tweets and their authors\n",
    "\n",
    "tweet_texts = []\n",
    "tweet_authors = []\n",
    "tweets_by_author = pd.DataFrame(index=names, columns=['combined'])\n",
    "\n",
    "\n",
    "for name in names:\n",
    "    tweets = [re.sub('https?:\\/\\/[-\\w.]\\.?[a-zA-Z0-9]+\\/?[a-zA-Z0-9]*', '', tweet.text).strip()\n",
    "              for tweet in all_tweets[name]]\n",
    "    tweets_by_author.loc[name, 'combined'] = ' '.join(tweets)\n",
    "    tweet_texts.extend(tweets)\n",
    "    tweet_authors.extend([name] * len(tweets))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame({'text': tweet_texts, 'author': tweet_authors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reality check: the tweets in the dataframe should change authors at a multiple of 500\n",
    "\n",
    "tweets_df.loc[995:1005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply tf-idf vectorization. Tweets are compact and rely on impact words more than grammatical constructions, so vocabulary seems more appropriate than tokenization and analysis involving parts of speech and phraseology. A question to be settled is whether individual tweets are too short to produce meaningful vectorizations, in which case groupings of several tweets might produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_vectors = vectorizer.fit_transform(tweets_by_author['combined'])\n",
    "terms = vectorizer.get_feature_names()\n",
    "cluster_df = pd.DataFrame(auth_vectors.toarray(), index=names, columns=terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-run the clustering algorithms on the vectorized combined tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.fit(normalize(cluster_df))\n",
    "ms.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "cluster_data = normalize(cluster_df)\n",
    "for _ in range(1000):\n",
    "    best_score = -999\n",
    "    num_labels = 0\n",
    "    for num_clusters in range(5, 10):\n",
    "        km = KMeans(num_clusters)\n",
    "        km.fit(cluster_data)\n",
    "        sil_score = silhouette_score(cluster_data, km.labels_)\n",
    "        if sil_score > best_score:\n",
    "            best_score = sil_score\n",
    "            num_labels = num_clusters\n",
    "    clusters.append(num_labels)\n",
    "\n",
    "optimal_num_clusters = Counter(clusters).most_common(1)[0][0]\n",
    "print(Counter(clusters).most_common())\n",
    "print('The optimal number of clusters is {}'.format(optimal_num_clusters))\n",
    "      \n",
    "best_labels1 = []\n",
    "best_score1 = -999\n",
    "centers1 = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    km = KMeans(optimal_num_clusters)\n",
    "    km.fit(cluster_data)\n",
    "    sil_score = silhouette_score(cluster_data, km.labels_)\n",
    "    if sil_score > best_score1:\n",
    "        best_score1 = sil_score\n",
    "        best_labels1 = km.labels_\n",
    "        centers1 = km.cluster_centers_\n",
    "\n",
    "for i in range(optimal_num_clusters):\n",
    "    print('Cluster {} comprises {}'.format(i, [name for ix, name in enumerate(names) if best_labels1[ix] == i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce feature space to 5 principal components and choose two with the greatest spread among authors for graphing\n",
    "svd = TruncatedSVD(5)\n",
    "cluster_lsa = svd.fit_transform(cluster_data)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components: \", total_variance*100)\n",
    "\n",
    "authors_by_component = pd.DataFrame(cluster_lsa, index=names)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(authors_by_component.loc[:,i].sort_values(ascending=False)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_component['label'] = best_labels1\n",
    "cmap = sns.color_palette('cubehelix', 7)\n",
    "ax = sns.scatterplot(data=authors_by_component, x=1, y=4, hue='label', palette=cmap)\n",
    "ax.set_title('Clusters plotted against 2 of 5 principal components')\n",
    "ax.set_ylabel('lsa4')\n",
    "ax.set_xlabel('lsa1')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters are fairly well segregated on a 2-component plot. Look for commonalities in the 10 highest-scoring terms for each author. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = cluster_df.T\n",
    "groupings = pd.DataFrame(index=names, columns=range(10))\n",
    "for name in names:\n",
    "    groupings.loc[name, :] = words[name].sort_values(ascending=False)[0:10].index\n",
    "groupings['label'] = best_labels1\n",
    "groupings.sort_values(by='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to see much in common among the members of each cluster, but both members of cluster 2 talk about leadership. All three authors in cluster 3 highlight deep learning, and the two cluster 6 autors have a shared interest in 5g and applied tech in general.  The other groups have no obvious commonalities among members (clusters 1 and 5) or are individuals (clusters 0 and 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for words closest to the centroid of each cluster that score as high-impact words for cluster members, and see which of these words all members have in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "centroids1 = centers1.argsort()[:, ::-1] \n",
    "words_df = pd.DataFrame(cluster_data, index=names, columns=terms)\n",
    "print('High-impact words close to the centroid and common to all members in each cluster\\n\\n')\n",
    "for i in range(optimal_num_clusters):\n",
    "    print('Cluster {}:'.format(i))\n",
    "    people = [name for name in names if groupings.loc[name, 'label'] == i]\n",
    "    words = []\n",
    "    for ind in centroids1[i]:\n",
    "        word = terms[ind]\n",
    "        common = True\n",
    "        for person in people:\n",
    "            if words_df.loc[person, word] <= 0.03: # the .03 threshold was obtained by trying various values\n",
    "                common = False\n",
    "                break\n",
    "        if common == True:\n",
    "            words.append(word)\n",
    "        if len(words) == 20:\n",
    "            break\n",
    "    print('\\n', words, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This programmatic analysis of the intra-group comonalities confirms and expands the insights from visual inspection of the top 10 words for each author.\n",
    "- Clusters 0 and 4, individuals, have large numbers of words, because neither has another person to filter for common words.\n",
    "- The words from cluster 1 words reveal no common topic of interest, but these are the authors who used few or no hashtags and mentions. So there is a logic to the grouping.\n",
    "- The cluster 2 authors both used the terms 'leadership' and 'workplace,' showing perhaps a shared interest in management.\n",
    "- For cluster 3, the authors had 'neuralnetworks' as well as 'deeplearning' in common, strengthening the topical bond.\n",
    "- There is a surprise in that the cluster 5 authors share many terms in common. Rather than topics, however, they seem to comprise mostly mentions and groups, suggesting these authors have overlapping work and/or social networks.\n",
    "- The cluster 6 authors shared 'edgecomputing' as well as '5g,' but also 'oracle,' enterprise software. These seem to suggest an applications-oriented focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__It should be noted__*, however, that a number of the common influential \"words\" are joined phrases, indicating that they likely were hashtags (or mentions). This contaminates the results, since, for instance, any number of authors might have written about 'deep learning,' even to the point where 'deep' and 'learning' on their own ended up with very low scores, or maybe were even excluded because they occurred too frequently. This seems an intractable problem with the tf-idf approach employed. A bag-of-words approach would have the converse problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thematic topic(s) of each cluster can be analyzed by looking at the terms closest to its centroid, regardless of which members used those terms or had them in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Words closest to the centroid of each cluster\\n\\n')\n",
    "for i in range(optimal_num_clusters):\n",
    "    print('Cluster {}:'.format(i))\n",
    "    words = [terms[ind] for ind in centroids1[i][:10]]\n",
    "    print(words, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results solidify the already established themes. Cluster 2 has 'hr' and 'hranalytics,' in addition to 'leadership' and 'workplace,' further emphasizing management concerns, but it also has 'odsc,' 'london,' '19,' 'keynote' and 'speaker,' suggesting the two authors in this group were strongly involved with the ODSC Europe conference held Sept. 19-22 this year in London. gains a sharper enterprise focus with the addition of 'sassoftware,' 'devops' and 'dataanalytics.' For cluster 3, 'datascientists,' 'digitaltransformation' and 'kdnuggets' extend the AI/machine learning emphasis of deep learning and neural networks. Similarly, 'emergingtech,' 'sassoftware,' 'dataanalytics' and 'devops' fit with the cluster 6 emphasis on applied data sciece and emerging technologies. The fact that there is little overlap between the central terms of clusters 1 and 5 and the high-scoring terms, respectively, of their members supports the idea that the connections here have more to do with things beside topical content  low use of hashtags and mentions for cluster 1 and network relationships for cluster 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SpectralClustering(n_clusters=optimal_num_clusters)\n",
    "\n",
    "best_score = -999\n",
    "best_sc_labels = []\n",
    "for _ in range(1000):\n",
    "    sc.fit(cluster_data)\n",
    "    sil_score = silhouette_score(cluster_data, sc.labels_)\n",
    "    if sil_score > best_score:\n",
    "        best_score = sil_score\n",
    "        best_sc_labels = sc.labels_\n",
    "\n",
    "\n",
    "print(sc_labels, '\\n')\n",
    "for i in range(7):\n",
    "    print('Cluster {} comprises {}'.format(i, [name for ix, name in enumerate(names) if sc_labels[ix] == i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are not as observably coherent as those from K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets_df['text'], tweets_df['author'], test_size=0.4,\n",
    "                                                    random_state=0, stratify=tweets_df['author'])\n",
    "\n",
    "\n",
    "# apply the vectorizer, but don't fit on the test set to prevent data leakage\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mcc_scorer = make_scorer(matthews_corrcoef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance with cross-validation scoring of the basic estimators, using the Matthews correlation coefficient, which uses all the metrics from the confusion matrix. See if classification into clusters (7 classes) produces significantly better results thn classifying by individual authors (15 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=20)\n",
    "\n",
    "print(cross_val_score(rfc, X_train_tfidf, y_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train_tfidf, y_train)\n",
    "print(matthews_corrcoef(y_test, rfc.predict(X_test_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create cluster targets\n",
    "\n",
    "groupings = dict(zip(names, best_labels))\n",
    "y_groupings_train = y_train.apply(lambda x: groupings[x])\n",
    "y_groupings_test = y_test.apply(lambda x: groupings[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(rfc, X_train_tfidf, y_groupings_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "print(cross_val_score(gbc, X_train_tfidf, y_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(gbc, X_train_tfidf, y_groupings_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = LogisticRegression()\n",
    "\n",
    "print(cross_val_score(lrc, X_train_tfidf, y_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(lrc, X_train_tfidf, y_groupings_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "print(cross_val_score(knc, X_train_tfidf, y_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_rf = AdaBoostClassifier(rfc)\n",
    "\n",
    "print(cross_val_score(ada_rf, X_train_tfidf, y_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_lr = AdaBoostClassifier(lrc)\n",
    "print(cross_val_score(ada_lr, X_train_tfidf, y_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cluster labels instead of individuals as the targets did not significantly improve the results, despite reducing the number of classes by half. The one exception to this was with gradient boosting, where the score increased by about 10%. Does it overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.fit(X_train_tfidf, y_train)\n",
    "print(matthews_corrcoef(y_test, gbc.predict(X_test_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.fit(X_train_tfidf, y_groupings_train)\n",
    "print(matthews_corrcoef(y_groupings_test, gbc.predict(X_test_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is not overfitting. How about logistic regression, which had similar performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc.fit(X_train_tfidf, y_train)\n",
    "print(matthews_corrcoef(y_test, lrc.predict(X_test_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc.fit(X_train_tfidf, y_groupings_train)\n",
    "print(matthews_corrcoef(y_groupings_test, lrc.predict(X_test_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither of the two best-performing algorithms is overfitting. See if lemmatizing the tweets improves performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(input):\n",
    "    tokens = nlp(input)\n",
    "    output = [token.lemma_ for token in tokens\n",
    "              if token.lemma_ not in ['#', \",'\", ',\"', \".'\", '.\"', \"?'\", '?\"']\n",
    "              and not (token.is_punct and len(token.lemma_) == 1) # keep two-character emoticons, e.g. :) or ;) \n",
    "              and not token.is_stop]\n",
    "    return ' '.join(output).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tokenized = X_train.apply(tokenize)\n",
    "X_test_tokenized = X_test.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tok_tfidf = vectorizer.fit_transform(X_train_tokenized)\n",
    "X_test_tok_tfidf = vectorizer.transform(X_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(lrc, X_train_tok_tfidf, y_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(lrc, X_train_tok_tfidf, y_groupings_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(gbc, X_train_tok_tfidf, y_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(gbc, X_train_tok_tfidf, y_groupings_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant improvement, and the cross-eval scores for gradient boosting are a little less consistent. At this point, I will choose logistic regression over gradient boosting, because performance is on a par and gradient boosting consumes much more compute resources, taking orders of magnitude longer. See if adding bigrams improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(1,2), \n",
    "                              max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                              min_df=2, # only use words that appear at least twice\n",
    "                              max_features=6000, # increase to ensure that at least the 1,500 highest-scoring bigrams are used\n",
    "                              stop_words='english', \n",
    "                              lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                              use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                              norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                              smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf2 = vectorizer2.fit_transform(X_train)\n",
    "X_test_tfidf2 = vectorizer2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(lrc, X_train_tfidf2, y_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(lrc, X_train_tfidf2, y_groupings_train, scoring=mcc_scorer, cv=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither tokenization nor adding bigrams significantly improved performance, so tune the hyperparameters for logistic regression on the basic tf-idf data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrc_params = [{'penalty': ['l1', 'l2'], 'tol': [1e-4, 1e-3, 1e-2], 'C': [3, 5, 7, 10],\n",
    "               'solver': ['liblinear'], 'multi_class': ['ovr']},\n",
    "              { 'penalty': ['l2'], 'tol': [1e-4, 1e-3, 1e-2], 'C': [3, 5, 7, 10],\n",
    "               'solver': ['newton-cg', 'lbfgs'], 'multi_class': ['ovr', 'multinomial']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(rand, X=tweets_df['text'], Y=tweets_df['author'], transformer=None,\n",
    "             clf=lrc, params=lrc_params, t_size=0.4):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=t_size, random_state=rand, stratify=Y)\n",
    "    if transformer:\n",
    "        X_train = transformer.fit_transform(X_train)\n",
    "        X_test = transformer.transform(X_test)\n",
    "    model = GridSearchCV(clf, param_grid=params, scoring=mcc_scorer)\n",
    "    model.fit(X_train, Y_train)\n",
    "    print('Optimal parameters: {}'.format(model.best_params_))\n",
    "    preds = model.predict(X_test)\n",
    "    print('Matthews score: {}'.format(matthews_corrcoef(Y_test, preds)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1 = optimize(rand=7, transformer=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt2 = optimize(rand=29, transformer=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt3 = optimize(rand=41, transformer=vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems relatively stable. Further tweaking could probably nail down the optimal C value between 4 and 5, and the tolerance between .oo1 and .0001, but the optimation consistently selected the liblinear solver with the L2 penalty and the one-vs-rest multi-class strategy. See if varying the train/test ratio can improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt4 = optimize(rand=7, transformer=vectorizer, t_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt5 = optimize(rand=7, transformer=vectorizer, t_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not overly sensitive to the relative sizes of the train/test split, although both 65/35 (best score) and 70/30 perform slightly better than 60/40. The decrease in the C parameter on the 70/30 split, however, might indicate that the model is starting to overfit, with larger regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f1 = f1_score(y_test, opt4.best_estimator_.predict(vectorizer.transform(X_test)), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the average F1 score, a more common metric, on the holdout test set for the 15-class predictions using the optimized logistic regression model is 0.89. The F1 scores for the individual classes ranged from 0.77 to 0.98, with 8 of the 15 classes scoring above .90 and only 1 scoring below .82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bin = label_binarize(y_test, classes=list(names))\n",
    "y_pred_bin = label_binarize(opt4.best_estimator_.predict(vectorizer.transform(X_test)), classes=list(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(names)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cycle(['darkorange', 'cornflowerblue', 'lime', 'deeppink', 'darkviolet'])\n",
    "plt.rcParams[\"figure.figsize\"] = [12.0, 6.0]\n",
    "\n",
    "for i, color in zip(range(len(names)), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='Class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curves for classification by individual authors')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curves and AUC-ROC scores shows that the optimal model performs very well on classifying the tweets by individual author. (Only one author has a score less than .90). However, the curves and scores seem perhaps too good. With more time, it would be worth constructing TOC curves, which include the true and false negative results as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try clustering with bigrams added into the tf-idf vector features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_vectors2 = vectorizer2.fit_transform(tweets_by_author['combined'])\n",
    "terms2 = vectorizer2.get_feature_names()\n",
    "cluster_df2 = pd.DataFrame(auth_vectors2.toarray(), index=names, columns=terms2)\n",
    "\n",
    "clusters = []\n",
    "cluster_data2 = normalize(cluster_df2)\n",
    "for _ in range(1000):\n",
    "    best_score = -999\n",
    "    num_labels = 0\n",
    "    for num_clusters in range(5, 10):\n",
    "        km = KMeans(num_clusters)\n",
    "        km.fit(cluster_data2)\n",
    "        sil_score = silhouette_score(cluster_data2, km.labels_)\n",
    "        if sil_score > best_score:\n",
    "            best_score = sil_score\n",
    "            num_labels = num_clusters\n",
    "    clusters.append(num_labels)\n",
    "\n",
    "optimal_num_clusters = Counter(clusters).most_common(1)[0][0]\n",
    "print(Counter(clusters).most_common())\n",
    "print('The optimal number of clusters is {}'.format(optimal_num_clusters))\n",
    "      \n",
    "best_labels = []\n",
    "best_score = -999\n",
    "\n",
    "for _ in range(1000):\n",
    "    km = KMeans(optimal_num_clusters)\n",
    "    km.fit(cluster_data2)\n",
    "    sil_score = silhouette_score(cluster_data2, km.labels_)\n",
    "    if sil_score > best_score:\n",
    "        best_score = sil_score\n",
    "        best_labels = km.labels_\n",
    "\n",
    "for i in range(optimal_num_clusters):\n",
    "    print('Cluster {} comprises {}'.format(i, [name for ix, name in enumerate(names) if best_labels[ix] == i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the individuals has been brought into the 'deep learning' group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last variation will be to match hashtags to concatenated bigrams, and add as many bigrams as matches to the combined tweets for each author. This will allow similarity measures to factor in matches between the bigram and hashtag variants of common two-word phrases, such as 'deep learning,' across authors. This will be used only for clustering, though, since hashtags vs. bigrams is a useful difference for classification purposes. It is expected that this could result in fewer, larger clusters by generating larger similarity scores between authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer3 = TfidfVectorizer(ngram_range=(2,2), \n",
    "                              max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                              min_df=2, # only use words that appear at least twice\n",
    "                              stop_words='english', \n",
    "                              lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                              use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                              norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                              smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer3.fit_transform(tweets_by_author['combined'])\n",
    "bigrams = vectorizer3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the bigrams\n",
    "\n",
    "def expand(author):\n",
    "    addition = []\n",
    "    for bigram in bigrams:\n",
    "        num_tags = all_tags[author][bigram.replace(' ', '')]\n",
    "        if num_tags:\n",
    "            addition.extend([bigram] * num_tags)\n",
    "    return ' ' + ' '.join(addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transfer = pd.DataFrame(index=names, columns=['extended'])\n",
    "for name in names:\n",
    "    transfer.loc[name, 'extended'] = tweets_by_author.loc[name, 'combined'] + expand(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_vectors3 = vectorizer2.fit_transform(transfer['extended'])\n",
    "terms3 = vectorizer2.get_feature_names()\n",
    "cluster_df3 = pd.DataFrame(auth_vectors3.toarray(), index=names, columns=terms3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "cluster_data3 = normalize(cluster_df3)\n",
    "for _ in range(1000):\n",
    "    best_score = -999\n",
    "    num_labels = 0\n",
    "    for num_clusters in range(3, 8):\n",
    "        km = KMeans(num_clusters)\n",
    "        km.fit(cluster_data3)\n",
    "        sil_score = silhouette_score(cluster_data3, km.labels_)\n",
    "        if sil_score > best_score:\n",
    "            best_score = sil_score\n",
    "            num_labels = num_clusters\n",
    "    clusters.append(num_labels)\n",
    "\n",
    "optimal_num_clusters = Counter(clusters).most_common(1)[0][0]\n",
    "print(Counter(clusters).most_common())\n",
    "print('The optimal number of clusters is {}'.format(optimal_num_clusters))\n",
    "      \n",
    "best_labels3 = []\n",
    "best_score3 = -999\n",
    "centers3 = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    km = KMeans(optimal_num_clusters)\n",
    "    km.fit(cluster_data3)\n",
    "    sil_score = silhouette_score(cluster_data3, km.labels_)\n",
    "    if sil_score > best_score3:\n",
    "        best_score3 = sil_score\n",
    "        best_labels3 = km.labels_\n",
    "        centers3 = km.cluster_centers_\n",
    "\n",
    "for i in range(optimal_num_clusters):\n",
    "    print('Cluster {} comprises {}'.format(i, [name for ix, name in enumerate(names) if best_labels3[ix] == i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique produces the most compact clustering. The number of clusters has come down to 5, with the other former individual joining the group that seems to be based on network connections. But EvanSinar, who despite using more than 600 hashtags and mentions initially was put in with the group that used few or none of these entities, now is grouped as an individual. This would support a hypothesis that this author used entities not used by the other authors, since \"double counting\" these as bigrams would isolate this person from both those that used more common entities and those that used few or none. It appears that hashtags and mentions dominated the clustering even when the full tweet texts were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids3 = centers3.argsort()[:, ::-1]\n",
    "words_df3 = pd.DataFrame(cluster_data3, index=names, columns=terms3)\n",
    "groupings3 = dict(zip(names, best_labels3))\n",
    "print('High-impact words close to the centroid and common to all members in each cluster\\n\\n')\n",
    "for i in range(optimal_num_clusters):\n",
    "    print('Cluster {}:'.format(i))\n",
    "    people = [name for name in names if groupings3[name] == i]\n",
    "    words = []\n",
    "    for ind in centroids3[i]:\n",
    "        word = terms3[ind]\n",
    "        common = True\n",
    "        for person in people:\n",
    "            if words_df3.loc[person, word] <= 0.001: # threshold lowered to produce results for cluster 0\n",
    "                common = False\n",
    "                break\n",
    "        if common == True:\n",
    "            words.append(word)\n",
    "        if len(words) == 20:\n",
    "            break\n",
    "    print('\\n', words, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the clusters grow larger, it is more difficult to find terms that all members have in common. But the terms closest to the cluster centers would still sketch common themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Words closest to the centroid of each cluster\\n\\n')\n",
    "for i in range(optimal_num_clusters):\n",
    "    print('Cluster {}:'.format(i))\n",
    "    words = [terms3[ind] for ind in centroids3[i][:10]]\n",
    "    print(words, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The former group that seemed to highlight management concerns but also might have had a stronger connection to the ODSC Europe conference has disappeared, with both members now assigned to other clusters. The largest now is the AI/machine learning group, cluster 0, comprising KirkDBorne, Ronald_vanLoon, BernardMarr, kdnuggets and analyticbridge. Next comes the cluster 1 group of four authors  AndrewYNg, mrogati, naval, hmason  who used few or no hashtags and mentions. The emerging technology group, cluster 4, now has three members: craigbrownphd, BigDataGal and tamaradull. Interestingly, cluster 2, the two authors seemingly grouped together via networking  bobehayes and data_nerd  now has some strong thematic content having to do with machine learning and AI. But it remains separate. Also interesting is that cluster 3, the only remaining group of one, EvanSinar, has an especially tight focus on data stories and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides its tighter thematic ties, this final 5-cluster grouping has a silhouette score of .032, which is 25% lower than the .040 score of the 7-cluster grouping based on the initial tf=idf vectorization of the tweet texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce feature space to 5 principal components and choose two with the greatest spread among authors for graphing\n",
    "\n",
    "cluster_lsa3 = svd.fit_transform(cluster_data3)\n",
    "\n",
    "authors_by_component3 = pd.DataFrame(cluster_lsa3, index=names)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(authors_by_component3.loc[:,i].sort_values(ascending=False)[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_explained3 = svd.explained_variance_ratio_\n",
    "for i in range(5):\n",
    "    print('Component {} explains {} of the total variance:'.format(i, variance_explained3[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: It seems odd that the first component explains less of the variance than any of the next 4.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_component3['label'] = best_labels3\n",
    "cmap = sns.color_palette('cubehelix', 5)\n",
    "ax = sns.scatterplot(data=authors_by_component3, x=1, y=3, hue='label', palette=cmap)\n",
    "ax.set_title('Clusters plotted against 2 of 5 principal components')\n",
    "ax.set_ylabel('lsa3')\n",
    "ax.set_xlabel('lsa1')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.savefig('cluster_plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final clustering, none of the clusters overlap even though the two components being graphed explain less than 20% of the total variance. This low explained variance also provides reasonable cause for the two points closest together in the graph, near (0.3, -0.4) being assigned to different clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
